{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c5cb3b-5207-4b62-b46c-157c25f962da",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'continue' not properly in loop (2803595026.py, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 57\u001b[0;36m\u001b[0m\n\u001b[0;31m    continue\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'continue' not properly in loop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.dom.minidom\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import tarfile\n",
    "import shutil\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "NYTimes Reference: https://catalog.ldc.upenn.edu/LDC2008T19\n",
    "\"\"\"\n",
    "\n",
    "sample_ratio = 0.02\n",
    "train_ratio = 0.7\n",
    "min_per_node = 200\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "source = []\n",
    "labels = []\n",
    "label_ids = []\n",
    "label_dict = {}\n",
    "sentence_ids = []\n",
    "hiera = defaultdict(set)\n",
    "ROOT_DIR = 'Nytimes/'\n",
    "label_f = 'nyt_label.vocab'\n",
    "\n",
    "\n",
    "# # 2003-07\n",
    "# def read_nyt(file_name):\n",
    "#     f = open(id_json, 'r')\n",
    "#     ids = f.readlines()\n",
    "#     f.close()\n",
    "    # print(ids[:2])\n",
    "    # f = open(label_f, 'r')\n",
    "    # label_vocab_s = f.readlines()\n",
    "    # f.close()\n",
    "    # label_vocab = []\n",
    "    # for label in label_vocab_s:\n",
    "    #     label = label.strip()\n",
    "    #     label_vocab.append(label)\n",
    "    # id_list = []\n",
    "    # for i in ids:\n",
    "    #     id_list.append(int(i[13:-5]))\n",
    "    # print(id_list[:2])\n",
    "    corpus = []\n",
    "\n",
    "# for file_name in tqdm(ids):\n",
    "xml_path = file_name.strip()\n",
    "try:\n",
    "    sample = {}\n",
    "    try:\n",
    "        dom = xml.dom.minidom.parse(xml_path)\n",
    "    except:\n",
    "        continue\n",
    "    root = dom.documentElement\n",
    "    tags = root.getElementsByTagName('p')\n",
    "    text = ''\n",
    "    for tag in tags[1:]:\n",
    "        text += tag.firstChild.data\n",
    "    if text == '':\n",
    "        continue\n",
    "    text = tokenizer.encode(text.lower(), truncation=True)\n",
    "    source.append(text)\n",
    "    sample_label = []\n",
    "    tags = root.getElementsByTagName('classifier')\n",
    "    for tag in tags:\n",
    "        type = tag.getAttribute('type')\n",
    "        if type != 'taxonomic_classifier':\n",
    "            continue\n",
    "        hier_path = tag.firstChild.data\n",
    "        hier_list = hier_path.split('/')\n",
    "        if len(hier_list) < 3:\n",
    "            continue\n",
    "        for l in range(1, len(hier_list) + 1):\n",
    "            label = '/'.join(hier_list[:l])\n",
    "            if label == 'Top':\n",
    "                continue\n",
    "            if label not in sample_label and label in label_vocab:\n",
    "                sample_label.append(label)\n",
    "    labels.append(sample_label)\n",
    "    sentence_ids.append(file_name)\n",
    "    sample['doc_topic'] = []\n",
    "    sample['doc_keyword'] = []\n",
    "    corpus.append(sample)\n",
    "except AssertionError:\n",
    "    print(xml_path)\n",
    "    print('Something went wrong...')\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7f2e0a6-9ac4-4d69-a8a7-cb7b5c1abeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one iteration of getting encoded text\n",
    "dom = xml.dom.minidom.parse(\"xml_bare_minimum.xml\")\n",
    "root = dom.documentElement\n",
    "tags = root.getElementsByTagName('p')\n",
    "text = \"\"\n",
    "for tag in tags:\n",
    "    # print(tag.firstChild.data)\n",
    "    text += tag.firstChild.data\n",
    "\n",
    "text = tokenizer.encode(text.lower(), truncation=True)\n",
    "source.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90ab150c-26e5-4453-b5b8-ddff751c28cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sample_label \u001b[38;5;129;01mand\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlabel_vocab\u001b[49m:\n\u001b[1;32m     17\u001b[0m     sample_label\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "tags = root.getElementsByTagName('classifier')\n",
    "for tag in tags:\n",
    "    type = tag.getAttribute('type')\n",
    "    if type != 'taxonomic_classifier':\n",
    "        continue\n",
    "    hier_path = tag.firstChild.data\n",
    "    # print(hier_path)\n",
    "    sample_label = []\n",
    "    hier_list = hier_path.split('/')\n",
    "    if len(hier_list) < 3:\n",
    "        continue\n",
    "    for l in range(1, len(hier_list) + 1):\n",
    "        label = '/'.join(hier_list[:l])\n",
    "        if label == 'Top':\n",
    "            continue\n",
    "        if label not in sample_label and label in label_vocab:\n",
    "            sample_label.append(label)\n",
    "            print(\"Label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPvenv",
   "language": "python",
   "name": "nlpvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
